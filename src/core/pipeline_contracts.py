"""
Pipeline Stage Contracts - E-02 Efficiency Implementation
========================================================

Pydantic-based data contracts for all cognitive pipeline stage handovers.
Eliminates context bloat by enforcing distilled, type-safe data exchange.

Design Principles:
- MINIMAL: Only essential data for next stage
- EXPLICIT: Clear data types and validation
- EFFICIENT: No nested JSON bloat or redundancy
- TRACEABLE: Every contract includes trace_id

Architecture:
Input → Stage Processing → Distilled Contract Output → Next Stage
"""

from pydantic import BaseModel, Field, model_validator
from typing import List, Dict, Any, Optional, Union, Protocol, runtime_checkable, Literal
from datetime import datetime
import uuid
from enum import Enum
from uuid import UUID

# Imports for infrastructure contracts
try:
    from .checkpoint_models import StateCheckpoint, AnalysisRevision
except Exception:
    # Soft import fallback for typing only
    StateCheckpoint = Any  # type: ignore
    AnalysisRevision = Any  # type: ignore


class AnalysisQuality(str, Enum):
    """Quality assessment levels"""
    EXCELLENT = "excellent"
    GOOD = "good" 
    ADEQUATE = "adequate"
    POOR = "poor"


class AnalysisConfidence(str, Enum):
    """Confidence levels for analysis outputs"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class StageStatus(str, Enum):
    """Execution status for pipeline stages"""
    SUCCESS = "success"
    PARTIAL = "partial"
    FAILED = "failed"


# ============================================================================
# EVIDENCE MODEL (Operation Hardening: Task #4)
# ============================================================================

class Evidence(BaseModel):
    """Structured, verifiable evidence for a specific claim"""
    claim: str = Field(..., max_length=500)
    source_quote: str = Field(..., max_length=1000)
    source_type: Literal["primary", "secondary"]
    source_url: Optional[str] = None
    source_document_id: Optional[str] = None
    credibility_weight: float = Field(..., ge=0.0, le=1.0)
    fetch_timestamp: str
    fetch_hash: str


# ============================================================================
# STAGE 1: SOCRATIC ENGINE CONTRACTS
# ============================================================================

class StrategicQuestion(BaseModel):
    """A single strategic clarification question"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4())[:8], description="Unique question identifier")
    question: str = Field(..., max_length=200, description="Concise strategic question")
    category: str = Field(..., description="Question category (success_definition, constraints, etc.)")
    priority: float = Field(..., ge=0.0, le=1.0, description="Priority score 0.0-1.0")
    tier: str = Field(..., description="Question tier (essential, strategic, expert)")
    tosca_tag: Optional[str] = Field(None, description="TOSCA element tag (trouble, owner, success, constraints, actors)")
    auto_generated: bool = Field(False, description="Whether this question was auto-generated for coverage")


class SocraticOutput(BaseModel):
    """Distilled output from the Socratic Engine - MINIMAL DATA ONLY"""

    # Core strategic questions (max 12 to support 3-4-3 tiered UX + buffer)
    key_strategic_questions: List[StrategicQuestion] = Field(
        ..., max_items=12, description="Essential strategic questions generated"
    )
    
    # Clarified problem statement (distilled from original query)
    clarified_problem_statement: str = Field(
        ..., max_length=500, description="Refined problem definition"
    )
    
    # Business context insights (extracted essentials only)
    key_business_insights: List[str] = Field(
        default=[], max_items=5, description="Critical business context points"
    )
    
    tosca_coverage: Dict[str, List[str]] = Field(
        default_factory=dict,
        description="Mapping of TOSCA elements to supporting questions"
    )
    missing_tosca_elements: List[str] = Field(
        default_factory=list,
        description="TOSCA elements lacking direct coverage"
    )
    autogenerated_question_ids: List[str] = Field(
        default_factory=list,
        description="IDs of auto-generated coverage questions"
    )

    # Execution metadata
    trace_id: str = Field(..., description="Pipeline trace identifier")
    quality_score: float = Field(..., ge=0.0, le=1.0, description="Output quality assessment")
    processing_time_ms: int = Field(..., description="Stage execution time")


# ============================================================================
# STAGE 2: PROBLEM STRUCTURING CONTRACTS
# ============================================================================

class MECEComponent(BaseModel):
    """A single MECE framework component"""
    dimension: str = Field(..., max_length=100)
    key_considerations: List[str] = Field(..., max_items=3)
    priority_level: int = Field(..., ge=1, le=3)


class ProblemStructuringOutput(BaseModel):
    """Distilled output from Problem Structuring Agent"""
    
    # MECE framework (simplified structure)
    mece_framework: List[MECEComponent] = Field(
        ..., max_items=5, description="Structured problem breakdown"
    )
    
    # Core assumptions identified
    core_assumptions: List[str] = Field(
        ..., max_items=5, description="Key assumptions requiring validation"
    )
    
    # Strategic constraints
    critical_constraints: List[str] = Field(
        default=[], max_items=4, description="Major limiting factors"
    )
    
    # Success criteria
    success_criteria: List[str] = Field(
        default=[], max_items=4, description="Measurable success indicators"
    )
    
    # Execution metadata
    trace_id: str
    quality_assessment: AnalysisQuality
    processing_time_ms: int
    autopad_used: bool = Field(False, description="Whether canonical MECE components were auto-added for coverage")
    autopadded_components: List[str] = Field(default_factory=list, description="Canonical MECE components that were auto-added")

    @model_validator(mode="before")
    @classmethod
    def _accept_legacy_fields(cls, data: Any) -> Any:
        """Accept legacy fields (e.g., framework_quality) and map to current schema.
        Provides sensible defaults for missing fields to maintain test fixture compatibility.
        """
        if not isinstance(data, dict):
            return data
        out = dict(data)
        # Map legacy 'framework_quality' -> 'quality_assessment'
        if "quality_assessment" not in out and "framework_quality" in out:
            try:
                q = str(out["framework_quality"]).strip().lower()
                mapping = {
                    "excellent": AnalysisQuality.EXCELLENT,
                    "good": AnalysisQuality.GOOD,
                    "adequate": AnalysisQuality.ADEQUATE,
                    "poor": AnalysisQuality.POOR,
                }
                out["quality_assessment"] = mapping.get(q, AnalysisQuality.ADEQUATE)
            except Exception:
                out["quality_assessment"] = AnalysisQuality.ADEQUATE
        # Default if still missing
        if "quality_assessment" not in out:
            out["quality_assessment"] = AnalysisQuality.ADEQUATE
        return out


# ============================================================================
# STAGE 3: HYBRID DATA RESEARCH (ORACLE) CONTRACTS - Operation Genesis
# ============================================================================

class OracleJobStatus(str, Enum):
    """Status of Oracle research job"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"


class OracleRequest(BaseModel):
    """Request for Oracle research"""
    query: str = Field(..., max_length=1000)
    context: Dict[str, Any] = Field(default_factory=dict)
    guardrails: Optional['OracleGuardrails'] = None
    trace_id: str


class OracleGuardrails(BaseModel):
    """Guardrails for Oracle research execution"""
    allowed_tools: List[Literal["cortex", "web_search", "chart"]] = Field(
        default_factory=lambda: ["cortex", "web_search"],
        description="Allowed research tools"
    )
    external_allowed: bool = Field(False, description="Whether external web research is permitted")
    budget_usd: Optional[float] = Field(0.10, ge=0.0, description="Budget cap in USD")
    max_steps: Optional[int] = Field(8, ge=1, description="Maximum planner steps")
    max_runtime_s: Optional[int] = Field(120, ge=30, description="Overall runtime cap in seconds")
    data_classification: Literal["PUBLIC", "INTERNAL", "CONFIDENTIAL", "RESTRICTED"] = Field(
        "INTERNAL", description="Data classification for retrieved content"
    )
    require_citations: bool = Field(True, description="Whether citations are mandatory")
    min_scores: Dict[str, float] = Field(default_factory=dict, description="Optional minimum score thresholds per metric")


class OracleCitation(BaseModel):
    """A single citation from Oracle research"""
    source_url: Optional[str] = None
    source_title: Optional[str] = None
    excerpt: Optional[str] = Field(
        None, max_length=500, description="Supporting quote or summary"
    )
    relevance_score: Optional[float] = Field(None, ge=0.0, le=1.0)
    timestamp: Optional[str] = None
    doc_id: Optional[str] = None
    uri: Optional[str] = None
    source: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

    @model_validator(mode="before")
    @classmethod
    def _normalize_fields(cls, data: Any) -> Any:
        """Normalize legacy citation keys from various providers."""
        if not isinstance(data, dict):
            return data

        normalized = dict(data)

        # Map legacy keys to canonical names
        if "url" in data and "source_url" not in data:
            normalized["source_url"] = data["url"]
        if "uri" in data and "source_url" not in normalized:
            normalized["source_url"] = data["uri"]
        if "title" in data and "source_title" not in data:
            normalized["source_title"] = data["title"]
        if "doc_id" in data:
            normalized.setdefault("metadata", {})["doc_id"] = data["doc_id"]
        if "cognitive_level" in data:
            normalized.setdefault("metadata", {})["cognitive_level"] = data["cognitive_level"]
        if "doc_type" in data:
            normalized.setdefault("metadata", {})["doc_type"] = data["doc_type"]

        return normalized

    class Config:
        extra = "allow"


class BriefingMemo(BaseModel):
    """Oracle research output - contextual intelligence briefing"""

    # High-level synthesis
    summary: Optional[str] = Field(
        None, description="Concise synthesis answering the user query"
    )
    structured_data: Optional[Dict[str, Any]] = Field(
        None, description="Structured metrics or context extracted from internal data"
    )

    # Key findings from research
    key_findings: List[str] = Field(
        default_factory=list, max_items=10, description="Critical insights from research"
    )

    # Citations and sources
    citations: List[OracleCitation] = Field(
        default_factory=list, max_items=25, description="Research sources"
    )

    # Expanded context for downstream stages
    web_findings: List[str] = Field(
        default_factory=list, description="Key insights captured from external research"
    )
    internal_context: List[str] = Field(
        default_factory=list, description="Important internal context items"
    )
    market_context: Optional[str] = Field(
        None, max_length=1000, description="Market and competitive context summary"
    )

    # Identified risks and opportunities
    risks_identified: List[str] = Field(default_factory=list, max_items=5)
    opportunities_identified: List[str] = Field(default_factory=list, max_items=5)

    # Research metadata
    gpa_scores: Dict[str, float] = Field(
        default_factory=dict, description="Planner quality metrics (plan_quality, groundedness, etc.)"
    )
    quality_indicator: Literal["GREEN", "YELLOW", "RED", "UNKNOWN"] = Field(
        "YELLOW", description="Overall quality flag from Oracle"
    )
    warnings: List[str] = Field(default_factory=list, description="Execution warnings or gaps")
    research_confidence: Optional[float] = Field(None, ge=0.0, le=1.0)
    research_depth: Optional[str] = Field(
        None, description="shallow, medium, deep - relative depth of research"
    )

    # Optional provider metadata (Perplexity / Oracle variants)
    provider: Optional[str] = Field(None, description="Research provider used")
    provider_model: Optional[str] = Field(None, description="Specific provider model")
    recency_metrics: Dict[str, int] = Field(default_factory=dict, description="Recency distribution of retrieved evidence")
    query_audit: List[Dict[str, Any]] = Field(default_factory=list, description="Internal audit trail for research queries")

    # Execution metadata
    trace_id: str
    processing_time_ms: Optional[int] = None
    status: Literal["COMPLETE", "PARTIAL", "FAILED"] = "COMPLETE"


# ============================================================================
# STAGE 4: CONSULTANT SELECTION CONTRACTS
# ============================================================================

class SelectedConsultant(BaseModel):
    """A selected consultant with minimal metadata"""
    consultant_id: str = Field(..., description="Consultant identifier")
    expertise_match_score: float = Field(..., ge=0.0, le=1.0)
    specialization: str = Field(..., max_length=100)
    selection_rationale: str = Field(..., max_length=200)


class ConsultantSelectionOutput(BaseModel):
    """Distilled output from Consultant Selection stage"""
    
    # Selected consultant team (typically 3)
    selected_consultants: List[SelectedConsultant] = Field(
        ..., min_items=1, max_items=5, description="Optimal consultant team"
    )
    
    # Team composition rationale
    team_rationale: str = Field(
        ..., max_length=300, description="Why this team composition"
    )
    
    # Team chemistry score
    team_synergy_score: float = Field(..., ge=0.0, le=1.0)
    
    # Cognitive diversity metrics
    diversity_score: float = Field(..., ge=0.0, le=1.0)
    
    # Execution metadata
    trace_id: str
    selection_confidence: AnalysisConfidence
    processing_time_ms: int


# ============================================================================
# STAGE 4: PARALLEL ANALYSIS CONTRACTS
# ============================================================================

class ConsultantAnalysis(BaseModel):
    """Individual consultant analysis result - OPERATION KEYSTONE: No length constraints, prompt enforces depth"""
    consultant_id: str
    key_insights: List[str] = Field(..., max_items=5, description="McKinsey-caliber strategic insights (800-1500 chars each)")
    risk_factors: List[str] = Field(default=[], max_items=3, description="Substantive risk analysis (500-800 chars each)")
    opportunities: List[str] = Field(default=[], max_items=3, description="Strategic opportunities (500-800 chars each)")
    recommendations: List[str] = Field(default=[], max_items=3, description="Actionable recommendations (800-1200 chars each)")
    confidence_level: AnalysisConfidence
    analysis_quality: AnalysisQuality


class ParallelAnalysisOutput(BaseModel):
    """Distilled output from Parallel Analysis stage"""
    
    # Individual consultant results
    consultant_analyses: List[ConsultantAnalysis] = Field(
        ..., min_items=1, max_items=5
    )
    
    # Cross-consultant insights
    convergent_insights: List[str] = Field(
        default=[], max_items=5, description="Insights agreed upon by multiple consultants"
    )
    
    divergent_perspectives: List[str] = Field(
        default=[], max_items=5, description="Areas of consultant disagreement"
    )
    
    # Analysis quality metrics
    overall_quality: AnalysisQuality
    insight_depth_score: float = Field(..., ge=0.0, le=1.0)
    
    # Execution metadata
    trace_id: str
    total_processing_time_ms: int
    successful_analyses: int
    
    # Operation Hardening: Orthogonality Watchdog
    orthogonality_index: float = Field(0.0, ge=0.0, le=1.0)
    minority_report: Optional[str] = None


# ============================================================================
# STAGE 5: DEVILS ADVOCATE CONTRACTS
# ============================================================================

class CriticalChallenge(BaseModel):
    """A critical challenge to the analysis - OPERATION KEYSTONE: Generous limits for depth"""
    challenge: str = Field(..., max_length=3000, description="Substantive challenge with strategic depth")
    severity: str = Field(..., description="high, medium, low")
    evidence: str = Field(..., max_length=2000, description="Evidence-backed reasoning")
    suggested_mitigation: str = Field(..., max_length=2000, description="Detailed mitigation strategy")


class DevilsAdvocateOutput(BaseModel):
    """Distilled output from Devils Advocate stage"""
    
    # Critical challenges identified
    key_challenges: List[CriticalChallenge] = Field(
        ..., max_items=5, description="Major challenges to the strategy"
    )
    
    # Assumption tests
    challenged_assumptions: List[str] = Field(
        default=[], max_items=5, description="Assumptions requiring further validation"
    )
    
    # Risk amplifications
    amplified_risks: List[str] = Field(
        default=[], max_items=4, description="Risks that may be underestimated"
    )
    
    # Overall critique strength
    critique_strength: AnalysisConfidence
    robustness_score: float = Field(..., ge=0.0, le=1.0)
    
    # Execution metadata
    trace_id: str
    processing_time_ms: int
    
    # Operation Hardening: DA Transcript & Assumption Diff
    da_transcript: Optional[str] = None
    assumption_diff: List[Dict[str, Any]] = Field(default_factory=list)


# ============================================================================
# STAGE 6: SENIOR ADVISOR CONTRACTS
# ============================================================================

class StrategicRecommendation(BaseModel):
    """A strategic recommendation with priority - OPERATION KEYSTONE: Generous limits, trust the prompt"""
    recommendation: str = Field(..., max_length=3000, description="McKinsey-level recommendation with strategic depth")
    priority: str = Field(..., description="critical, important, suggested")
    rationale: str = Field(..., max_length=5000, description="Comprehensive evidence-backed rationale (Pyramid Principle)")
    implementation_complexity: str = Field(..., description="low, medium, high")
    implementation_guidance: Optional[str] = Field(None, max_length=8000, description="Detailed implementation roadmap with MECE steps")


class ConfidenceAssessment(BaseModel):
    """Structured, verifiable confidence assessment for final recommendations"""
    probability_of_success: float = Field(..., ge=0.0, le=1.0)
    confidence_band: float = Field(..., ge=0.0, le=1.0)
    key_uncertainties: List[str] = Field(default_factory=list, max_items=6)
    critical_triggers: List[str] = Field(default_factory=list, max_items=6)


class SeniorAdvisorOutput(BaseModel):
    """Distilled output from Senior Advisor synthesis"""
    
    # Final strategic recommendations
    strategic_recommendations: List[StrategicRecommendation] = Field(
        ..., min_items=1, max_items=6, description="Final strategic recommendations"
    )
    
    # Executive summary - OPERATION KEYSTONE: Generous limit for McKinsey-depth synthesis
    executive_summary: str = Field(
        ..., max_length=8000, description="Comprehensive executive summary with strategic depth (Pyramid Principle)"
    )

    # Key decision points
    critical_decisions: List[str] = Field(
        default=[], max_items=4, description="Critical decisions requiring leadership attention"
    )

    # Risk/opportunity balance - OPERATION KEYSTONE: Generous limit
    risk_opportunity_assessment: str = Field(
        ..., max_length=5000, description="Comprehensive risk/opportunity analysis with evidence"
    )
    
    # Quality and confidence
    synthesis_quality: AnalysisQuality
    overall_confidence: AnalysisConfidence
    confidence_assessment: Optional[ConfidenceAssessment] = None

    # Structured evidence
    evidence: List[Evidence] = Field(
        default=[], max_items=10, description="Structured evidence supporting recommendations"
    )

    # Execution metadata
    trace_id: str
    processing_time_ms: int


# ============================================================================
# OPERATION AEGIS: GOALS LEDGER (Manus.im Best Practice)
# ============================================================================

class StrategicGoal(BaseModel):
    """
    A single high-level strategic goal for the analysis.

    OPERATION AEGIS: Goals ledger for attention manipulation via recitation.
    Implements Manus.im best practice: "Keep/refresh a lightweight todo.md
    (or goals ledger) and recite it into the tail of context to fight
    'lost-in-the-middle' drift on long loops."
    """
    id: str = Field(default_factory=lambda: str(uuid.uuid4())[:8], description="Unique goal identifier")
    description: str = Field(..., max_length=200, description="Clear goal statement")
    priority: int = Field(..., ge=1, le=3, description="Priority level (1=highest, 3=lowest)")
    status: Literal["active", "completed", "blocked"] = Field(
        default="active",
        description="Current status of the goal"
    )
    assigned_stage: Optional[str] = Field(
        None,
        description="Stage responsible for this goal (if any)"
    )

    def to_recitation_string(self) -> str:
        """Format goal for context recitation"""
        status_icon = {"active": "⏳", "completed": "✅", "blocked": "🚫"}[self.status]
        priority_icon = {1: "🔴", 2: "🟡", 3: "🟢"}[self.priority]
        return f"{status_icon} {priority_icon} [{self.id}] {self.description}"


# ============================================================================
# PIPELINE STATE MANAGEMENT
# ============================================================================

class PipelineState(BaseModel):
    """Complete pipeline state with distilled stage outputs"""

    # Pipeline initialization
    initial_query: str = Field(..., description="Original user query")
    trace_id: str = Field(..., description="Pipeline execution trace ID")
    started_at: datetime = Field(default_factory=datetime.utcnow)

    # OPERATION AEGIS: Goals ledger for attention manipulation
    active_goals: List[StrategicGoal] = Field(
        default_factory=list,
        description="Strategic goals for the analysis (Operation Aegis)"
    )

    # Stage outputs (populated as pipeline progresses)
    socratic_results: Optional[SocraticOutput] = None
    structuring_results: Optional[ProblemStructuringOutput] = None
    interaction_sweep_results: Optional[Any] = None  # InteractionSweepOutput - forward reference
    briefing_memo: Optional[BriefingMemo] = None  # Operation Genesis: Oracle research output
    enhancement_research_results: Optional[List[Dict[str, Any]]] = None  # Progressive Questions: Research answers
    selection_results: Optional[ConsultantSelectionOutput] = None
    analysis_results: Optional[ParallelAnalysisOutput] = None
    critique_results: Optional[DevilsAdvocateOutput] = None
    final_results: Optional[SeniorAdvisorOutput] = None

    # Cross-stage metadata
    enhancement_metadata: Dict[str, Any] = Field(default_factory=dict, description="Metadata from query enhancement stage")
    complexity_level: Optional[str] = Field(None, description="Pipeline complexity classification (low/medium/high)")
    processing_mode: Optional[str] = Field(None, description="Pipeline processing mode (light/standard/full)")

    # Pipeline metrics
    total_processing_time_ms: int = 0
    stages_completed: int = 0
    overall_quality: Optional[AnalysisQuality] = None
    
    def get_completed_stages(self) -> List[str]:
        """Get list of completed pipeline stages"""
        completed = []
        if self.socratic_results: completed.append("socratic")
        if self.structuring_results: completed.append("structuring") 
        if self.selection_results: completed.append("selection")
        if self.analysis_results: completed.append("analysis")
        if self.critique_results: completed.append("critique")
        if self.final_results: completed.append("synthesis")
        return completed
    
    def is_complete(self) -> bool:
        """Check if pipeline execution is complete"""
        return self.final_results is not None


# ============================================================================
# FINAL OUTPUT CONTRACT
# ============================================================================

class FinalReport(BaseModel):
    """Final user-facing output - LIGHTWEIGHT with pointers to full data"""
    
    # Core deliverable
    executive_summary: str = Field(..., max_length=1500)
    strategic_recommendations: List[StrategicRecommendation]
    
    # Key insights and decisions
    critical_insights: List[str] = Field(..., max_items=5)
    key_decisions_required: List[str] = Field(..., max_items=4)
    
    # Quality assurance
    analysis_confidence: AnalysisConfidence
    recommendation_strength: AnalysisQuality
    confidence_assessment: Optional[ConfidenceAssessment] = None
    
    # Audit trail (POINTER to full data, not the data itself)
    full_trajectory_log_path: Optional[str] = Field(
        None, description="Path to complete analysis logs"
    )
    detailed_evidence_path: Optional[str] = Field(
        None, description="Path to detailed evidence and reasoning"
    )
    
    # Execution summary
    trace_id: str
    total_processing_time_ms: int
    pipeline_efficiency_score: float = Field(..., ge=0.0, le=1.0)
    
    # Operation Hardening: DA Transcript & Assumption Diff surfaced to UI
    da_transcript: Optional[str] = None
    assumption_diff: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Operation Hardening: Orthogonality Watchdog metric surfaced to footer
    orthogonality_index: float = Field(0.0, ge=0.0, le=1.0)


# ============================================================================
# STAGE 7: INTERACTION SWEEP (post-MECE)
# ============================================================================

class InteractionSweepOutput(BaseModel):
    """Cross-term risks discovered between MECE components"""
    cross_term_risks: List[str] = Field(default_factory=list, max_items=5)


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def validate_stage_output(output: Any, expected_type: type) -> bool:
    """Validate that a stage output matches expected contract type"""
    try:
        if not isinstance(output, expected_type):
            return False
        return True
    except Exception:
        return False


def calculate_pipeline_efficiency(state: PipelineState) -> float:
    """Calculate overall pipeline efficiency score"""
    if not state.is_complete():
        return 0.0
    
    # Factor in quality, time, and completeness
    quality_score = 1.0 if state.overall_quality == AnalysisQuality.EXCELLENT else 0.8
    time_efficiency = min(1.0, 300000 / max(state.total_processing_time_ms, 1))  # 5min baseline
    completeness = len(state.get_completed_stages()) / 6.0
    
    return (quality_score * 0.5 + time_efficiency * 0.3 + completeness * 0.2)


# ============================================================================
# ATLAS PHASE 1: INFRASTRUCTURE & EXECUTION PROTOCOLS
# ============================================================================

@runtime_checkable
class ICheckpointRepository(Protocol):
    """Abstracts persistence for checkpoints and revisions."""

    async def save_checkpoint(self, checkpoint: StateCheckpoint) -> StateCheckpoint: ...

    async def load_checkpoint(self, checkpoint_id: UUID) -> Optional[StateCheckpoint]: ...

    async def load_checkpoints_for_trace(self, trace_id: UUID) -> List[StateCheckpoint]: ...

    async def save_revision(self, revision: AnalysisRevision) -> AnalysisRevision: ...

    async def update_revision(self, revision: AnalysisRevision) -> AnalysisRevision: ...

    async def count_checkpoints_for_trace(self, trace_id: UUID) -> int: ...


@runtime_checkable
class IRevisionService(Protocol):
    """Manages creation of immutable analysis branches (revisions)."""

    async def create_revision_branch(
        self,
        parent_trace_id: UUID,
        checkpoint_id: UUID,
        revision_data: Dict[str, Any],
        revision_rationale: Optional[str] = None,
        user_id: Optional[UUID] = None,
        session_id: Optional[UUID] = None,
    ) -> UUID: ...


@runtime_checkable
class IStageExecutor(Protocol):
    """Executes a pipeline stage, transforming PipelineState -> PipelineState."""

    async def execute(self, state: PipelineState) -> PipelineState: ...
