"""
Tool Decision Framework for METIS V4 Single Agent Architecture

Implements the "tools-as-structured-decisions" pattern where:
1. LLM generates structured decision documents
2. Deterministic code executes the decisions
3. Full traceability and error handling throughout

This separates cognitive decision-making (LLM) from execution (deterministic),
enabling better reliability, testing, and transparency.
"""

import json
import logging
import time
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field, asdict
from enum import Enum

from src.core.unified_context_stream import UnifiedContextStream, ContextEventType
from src.core.events.event_emitters import ToolEventEmitter
from src.core.incremental_context_manager import IncrementalContextManager


# Stub classes for optional dependencies
@dataclass
class ToolExecutionResult:
    """Stub for ToolExecutionResult"""

    success: bool = True
    result: Any = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class MCPMentalModelTool:
    """Stub for MCPMentalModelTool"""

    def __init__(self, name: str):
        self.name = name

    async def execute(self, **kwargs) -> ToolExecutionResult:
        return ToolExecutionResult(success=True, result=f"Mock result for {self.name}")


class MCPToolRegistry:
    """Stub for MCPToolRegistry"""

    def __init__(self):
        self.tools = {}

    def get_tool(self, name: str) -> Optional[MCPMentalModelTool]:
        return MCPMentalModelTool(name)

    def list_tools(self) -> List[str]:
        return ["mock_tool_1", "mock_tool_2"]


class UnifiedLLMClient:
    """Stub for UnifiedLLMClient"""

    async def generate_structured_response(
        self, prompt: str, **kwargs
    ) -> Dict[str, Any]:
        return {"decision": "mock_decision", "confidence": 0.8}


logger = logging.getLogger(__name__)


class ToolDecisionType(str, Enum):
    """Types of tool decisions that can be made"""

    SINGLE_TOOL = "single_tool"  # Execute one tool
    PARALLEL_TOOLS = "parallel_tools"  # Execute multiple tools in parallel
    SEQUENTIAL_TOOLS = "sequential_tools"  # Execute tools in sequence
    CONDITIONAL_TOOL = "conditional_tool"  # Execute tool based on conditions
    COMPOSITE_TOOL = "composite_tool"  # Complex tool execution pattern


class ToolDecisionStatus(str, Enum):
    """Status of tool decision processing"""

    GENERATED = "generated"  # Decision document created
    VALIDATED = "validated"  # Decision passed validation
    EXECUTING = "executing"  # Decision being executed
    COMPLETED = "completed"  # Decision execution completed
    FAILED = "failed"  # Decision execution failed
    CANCELLED = "cancelled"  # Decision execution cancelled


class ToolExecutionStrategy(str, Enum):
    """Execution strategies for tool decisions"""

    IMMEDIATE = "immediate"  # Execute immediately
    DEFERRED = "deferred"  # Execute later
    CONDITIONAL = "conditional"  # Execute if conditions met
    RETRY_ON_FAILURE = "retry_on_failure"  # Retry failed executions


@dataclass
class ToolParameterDecision:
    """Decision about a specific tool parameter"""

    parameter_name: str
    value: Any
    confidence: float  # 0.0-1.0
    source: str  # "llm", "context", "default", "computed"
    reasoning: str
    alternatives: List[Any] = field(default_factory=list)
    constraints: List[str] = field(default_factory=list)


@dataclass
class ToolSelectionDecision:
    """Decision about tool selection and configuration"""

    tool_name: str
    confidence: float  # 0.0-1.0
    reasoning: str

    # Parameter decisions
    parameters: Dict[str, ToolParameterDecision] = field(default_factory=dict)

    # Execution context
    context_requirements: List[str] = field(default_factory=list)
    expected_output_type: Optional[str] = None
    success_criteria: List[str] = field(default_factory=list)

    # Risk assessment
    risk_factors: List[str] = field(default_factory=list)
    mitigation_strategies: List[str] = field(default_factory=list)

    # Alternatives
    alternative_tools: List[str] = field(default_factory=list)
    fallback_strategy: Optional[str] = None


@dataclass
class ToolDecisionDocument:
    """Complete structured decision document generated by LLM"""

    decision_id: str
    decision_type: ToolDecisionType
    strategy: ToolExecutionStrategy

    # Core decision
    primary_tool: ToolSelectionDecision

    # Decision context (required fields)
    context_analysis: str
    decision_reasoning: str
    confidence_level: float  # 0.0-1.0

    # Secondary tools (optional)
    secondary_tools: List[ToolSelectionDecision] = field(default_factory=list)

    # Execution plan
    execution_order: List[str] = field(default_factory=list)
    parallelization_groups: List[List[str]] = field(default_factory=list)
    dependencies: Dict[str, List[str]] = field(default_factory=dict)

    # Risk and validation
    validation_checks: List[str] = field(default_factory=list)
    rollback_plan: Optional[str] = None
    timeout_seconds: int = 300

    # Metadata
    created_at: datetime = field(default_factory=datetime.utcnow)
    engagement_id: Optional[str] = None
    phase: Optional[str] = None
    llm_model_used: Optional[str] = None
    token_usage: Optional[Dict[str, int]] = None


@dataclass
class ToolDecisionValidationResult:
    """Result of validating a tool decision document"""

    is_valid: bool
    validation_score: float  # 0.0-1.0

    # Validation details
    passed_checks: List[str] = field(default_factory=list)
    failed_checks: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

    # Parameter validation
    parameter_issues: Dict[str, List[str]] = field(default_factory=dict)
    missing_requirements: List[str] = field(default_factory=list)

    # Risk assessment
    risk_level: str = "medium"  # low, medium, high, critical
    requires_human_approval: bool = False

    # Suggestions
    recommended_changes: List[str] = field(default_factory=list)
    alternative_approaches: List[str] = field(default_factory=list)


@dataclass
class ToolDecisionExecutionResult:
    """Result of executing a tool decision document"""

    decision_id: str
    execution_id: str
    status: ToolDecisionStatus

    # Results
    tool_results: Dict[str, ToolExecutionResult] = field(default_factory=dict)
    aggregated_output: Optional[Dict[str, Any]] = None

    # Performance
    total_execution_time_ms: float = 0.0
    tool_execution_times: Dict[str, float] = field(default_factory=dict)

    # Status tracking
    started_at: datetime = field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None

    # Error handling
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    retry_count: int = 0

    # Context
    execution_context: Dict[str, Any] = field(default_factory=dict)
    traceability_data: Dict[str, Any] = field(default_factory=dict)


class ToolDecisionValidator:
    """Validates tool decision documents for safety and correctness"""

    def __init__(self, tool_registry: MCPToolRegistry):
        self.logger = logging.getLogger(f"{__name__}.ToolDecisionValidator")
        self.tool_registry = tool_registry

        # Validation rules
        self.max_parallel_tools = 5
        self.max_total_tools = 10
        self.max_timeout_seconds = 600
        self.min_confidence_threshold = 0.3

    async def validate_decision(
        self, decision: ToolDecisionDocument, context: Optional[Dict[str, Any]] = None
    ) -> ToolDecisionValidationResult:
        """Validate a tool decision document"""
        validation_result = ToolDecisionValidationResult(
            is_valid=True, validation_score=1.0
        )

        try:
            # Basic structure validation
            await self._validate_structure(decision, validation_result)

            # Tool availability validation
            await self._validate_tool_availability(decision, validation_result)

            # Parameter validation
            await self._validate_parameters(decision, validation_result)

            # Execution plan validation
            await self._validate_execution_plan(decision, validation_result)

            # Risk assessment
            await self._assess_risks(decision, validation_result, context)

            # Calculate final validation score
            validation_result.validation_score = self._calculate_validation_score(
                validation_result
            )

            # Determine if valid
            validation_result.is_valid = (
                len(validation_result.failed_checks) == 0
                and validation_result.validation_score >= 0.7
            )

            self.logger.info(
                f"‚úÖ Decision validation: {decision.decision_id} | "
                f"Valid: {validation_result.is_valid} | "
                f"Score: {validation_result.validation_score:.2f}"
            )

            return validation_result

        except Exception as e:
            self.logger.error(f"‚ùå Validation error for {decision.decision_id}: {e}")
            validation_result.is_valid = False
            validation_result.failed_checks.append(f"Validation exception: {str(e)}")
            return validation_result

    async def _validate_structure(
        self, decision: ToolDecisionDocument, result: ToolDecisionValidationResult
    ):
        """Validate basic decision structure"""
        checks = []

        # Required fields
        if not decision.decision_id:
            result.failed_checks.append("Missing decision_id")
        else:
            checks.append("decision_id_present")

        if not decision.primary_tool.tool_name:
            result.failed_checks.append("Missing primary tool name")
        else:
            checks.append("primary_tool_specified")

        if decision.confidence_level < self.min_confidence_threshold:
            result.failed_checks.append(
                f"Confidence too low: {decision.confidence_level}"
            )
        else:
            checks.append("confidence_adequate")

        # Timeout validation
        if decision.timeout_seconds > self.max_timeout_seconds:
            result.failed_checks.append(
                f"Timeout too long: {decision.timeout_seconds}s"
            )
        else:
            checks.append("timeout_reasonable")

        # Tool count limits
        total_tools = 1 + len(decision.secondary_tools)
        if total_tools > self.max_total_tools:
            result.failed_checks.append(f"Too many tools: {total_tools}")
        else:
            checks.append("tool_count_within_limits")

        result.passed_checks.extend(checks)

    async def _validate_tool_availability(
        self, decision: ToolDecisionDocument, result: ToolDecisionValidationResult
    ):
        """Validate that requested tools exist and are available"""
        checks = []

        # Check primary tool
        primary_tool = self.tool_registry.get_tool(decision.primary_tool.tool_name)
        if not primary_tool:
            result.failed_checks.append(
                f"Primary tool not found: {decision.primary_tool.tool_name}"
            )
        else:
            checks.append("primary_tool_available")

        # Check secondary tools
        for tool_decision in decision.secondary_tools:
            tool = self.tool_registry.get_tool(tool_decision.tool_name)
            if not tool:
                result.failed_checks.append(
                    f"Secondary tool not found: {tool_decision.tool_name}"
                )
            else:
                checks.append(f"secondary_tool_available_{tool_decision.tool_name}")

        result.passed_checks.extend(checks)

    async def _validate_parameters(
        self, decision: ToolDecisionDocument, result: ToolDecisionValidationResult
    ):
        """Validate tool parameters"""
        # Validate primary tool parameters
        await self._validate_tool_parameters(decision.primary_tool, result, "primary")

        # Validate secondary tool parameters
        for i, tool_decision in enumerate(decision.secondary_tools):
            await self._validate_tool_parameters(
                tool_decision, result, f"secondary_{i}"
            )

    async def _validate_tool_parameters(
        self,
        tool_decision: ToolSelectionDecision,
        result: ToolDecisionValidationResult,
        prefix: str,
    ):
        """Validate parameters for a specific tool"""
        tool = self.tool_registry.get_tool(tool_decision.tool_name)
        if not tool:
            return

        tool_schema = tool.schema
        issues = []

        # Check required parameters
        required_params = [p for p in tool_schema.parameters if p.required]
        for param in required_params:
            if param.name not in tool_decision.parameters:
                issues.append(f"Missing required parameter: {param.name}")
            else:
                result.passed_checks.append(f"{prefix}_param_{param.name}_provided")

        # Validate parameter values
        for param_name, param_decision in tool_decision.parameters.items():
            # Find parameter schema
            param_schema = next(
                (p for p in tool_schema.parameters if p.name == param_name), None
            )

            if not param_schema:
                issues.append(f"Unknown parameter: {param_name}")
                continue

            # Type validation would go here
            # Range validation would go here
            # Enum validation would go here

            result.passed_checks.append(f"{prefix}_param_{param_name}_valid")

        if issues:
            result.parameter_issues[tool_decision.tool_name] = issues

    async def _validate_execution_plan(
        self, decision: ToolDecisionDocument, result: ToolDecisionValidationResult
    ):
        """Validate the execution plan"""
        checks = []

        # Check parallelization limits
        for group in decision.parallelization_groups:
            if len(group) > self.max_parallel_tools:
                result.failed_checks.append(
                    f"Too many parallel tools in group: {len(group)}"
                )
            else:
                checks.append("parallelization_within_limits")

        # Check dependencies for cycles
        if self._has_dependency_cycles(decision.dependencies):
            result.failed_checks.append("Circular dependencies detected")
        else:
            checks.append("no_circular_dependencies")

        result.passed_checks.extend(checks)

    def _has_dependency_cycles(self, dependencies: Dict[str, List[str]]) -> bool:
        """Check for circular dependencies using DFS"""
        visited = set()
        rec_stack = set()

        def has_cycle(node):
            if node not in visited:
                visited.add(node)
                rec_stack.add(node)

                for neighbor in dependencies.get(node, []):
                    if neighbor not in visited:
                        if has_cycle(neighbor):
                            return True
                    elif neighbor in rec_stack:
                        return True

                rec_stack.remove(node)
            return False

        for node in dependencies:
            if has_cycle(node):
                return True
        return False

    async def _assess_risks(
        self,
        decision: ToolDecisionDocument,
        result: ToolDecisionValidationResult,
        context: Optional[Dict[str, Any]] = None,
    ):
        """Assess risks and determine approval requirements"""
        risk_score = 0.0

        # High-risk tool combinations
        if decision.decision_type == ToolDecisionType.PARALLEL_TOOLS:
            risk_score += 0.2

        # Low confidence decisions
        if decision.confidence_level < 0.6:
            risk_score += 0.3

        # Long timeouts
        if decision.timeout_seconds > 120:
            risk_score += 0.1

        # Many tools
        if len(decision.secondary_tools) > 3:
            risk_score += 0.2

        # Assign risk level
        if risk_score >= 0.7:
            result.risk_level = "critical"
            result.requires_human_approval = True
        elif risk_score >= 0.5:
            result.risk_level = "high"
            result.requires_human_approval = True
        elif risk_score >= 0.3:
            result.risk_level = "medium"
        else:
            result.risk_level = "low"

    def _calculate_validation_score(
        self, result: ToolDecisionValidationResult
    ) -> float:
        """Calculate overall validation score"""
        total_checks = len(result.passed_checks) + len(result.failed_checks)
        if total_checks == 0:
            return 0.5

        base_score = len(result.passed_checks) / total_checks

        # Penalties
        if result.parameter_issues:
            base_score -= 0.1 * len(result.parameter_issues)

        if result.warnings:
            base_score -= 0.05 * len(result.warnings)

        return max(0.0, min(1.0, base_score))


class DeterministicToolExecutor:
    """Executes validated tool decisions deterministically"""

    def __init__(
        self, tool_registry: MCPToolRegistry, context_stream: UnifiedContextStream
    ):
        self.logger = logging.getLogger(f"{__name__}.DeterministicToolExecutor")
        self.tool_registry = tool_registry
        self.context_stream = context_stream
        self.cache = None  # get_performance_cache()
        self.tool_events = ToolEventEmitter(
            context_stream,
            default_metadata={"component": "DeterministicToolExecutor"},
        )

        # Execution state
        self.active_executions: Dict[str, ToolDecisionExecutionResult] = {}

    async def execute_decision(
        self,
        decision: ToolDecisionDocument,
        context: Optional[Dict[str, Any]] = None,
        validation_result: Optional[ToolDecisionValidationResult] = None,
    ) -> ToolDecisionExecutionResult:
        """Execute a validated tool decision"""
        execution_id = f"exec_{decision.decision_id}_{int(time.time() * 1000)}"
        start_time = time.time()

        # Initialize execution result
        execution_result = ToolDecisionExecutionResult(
            decision_id=decision.decision_id,
            execution_id=execution_id,
            status=ToolDecisionStatus.EXECUTING,
            execution_context=context or {},
        )

        self.active_executions[execution_id] = execution_result

        try:
            # Log start of execution
            await self._log_execution_event(
                decision,
                execution_result,
                "execution_started",
                {
                    "validation_result": (
                        asdict(validation_result) if validation_result else None
                    )
                },
            )

            # Execute based on decision type
            if decision.decision_type == ToolDecisionType.SINGLE_TOOL:
                await self._execute_single_tool(decision, execution_result, context)

            elif decision.decision_type == ToolDecisionType.PARALLEL_TOOLS:
                await self._execute_parallel_tools(decision, execution_result, context)

            elif decision.decision_type == ToolDecisionType.SEQUENTIAL_TOOLS:
                await self._execute_sequential_tools(
                    decision, execution_result, context
                )

            elif decision.decision_type == ToolDecisionType.CONDITIONAL_TOOL:
                await self._execute_conditional_tool(
                    decision, execution_result, context
                )

            else:
                raise ValueError(f"Unsupported decision type: {decision.decision_type}")

            # Aggregate results
            execution_result.aggregated_output = await self._aggregate_results(
                execution_result.tool_results, decision
            )

            # Mark as completed
            execution_result.status = ToolDecisionStatus.COMPLETED
            execution_result.completed_at = datetime.utcnow()
            execution_result.total_execution_time_ms = (time.time() - start_time) * 1000

            await self._log_execution_event(
                decision,
                execution_result,
                "execution_completed",
                {"total_time_ms": execution_result.total_execution_time_ms},
            )

            self.logger.info(
                f"‚úÖ Tool decision executed: {decision.decision_id} | "
                f"Time: {execution_result.total_execution_time_ms:.1f}ms | "
                f"Tools: {len(execution_result.tool_results)}"
            )

        except Exception as e:
            execution_result.status = ToolDecisionStatus.FAILED
            execution_result.errors.append(str(e))
            execution_result.completed_at = datetime.utcnow()
            execution_result.total_execution_time_ms = (time.time() - start_time) * 1000

            await self._log_execution_event(
                decision, execution_result, "execution_failed", {"error": str(e)}
            )

            self.logger.error(
                f"‚ùå Tool decision execution failed: {decision.decision_id} | Error: {e}"
            )

        finally:
            # Clean up active execution
            if execution_id in self.active_executions:
                del self.active_executions[execution_id]

        return execution_result

    async def _execute_single_tool(
        self,
        decision: ToolDecisionDocument,
        execution_result: ToolDecisionExecutionResult,
        context: Optional[Dict[str, Any]],
    ):
        """Execute a single tool"""
        tool_decision = decision.primary_tool
        tool_start_time = time.time()

        # Execute the tool
        result = await self.tool_registry.execute_tool(
            tool_name=tool_decision.tool_name,
            parameters={k: v.value for k, v in tool_decision.parameters.items()},
            context=context,
            engagement_id=decision.engagement_id,
        )

        execution_time = (time.time() - tool_start_time) * 1000

        # Store results
        execution_result.tool_results[tool_decision.tool_name] = result
        execution_result.tool_execution_times[tool_decision.tool_name] = execution_time

        if not result.success:
            execution_result.errors.append(
                f"Tool {tool_decision.tool_name} failed: {result.error_message}"
            )

    async def _execute_parallel_tools(
        self,
        decision: ToolDecisionDocument,
        execution_result: ToolDecisionExecutionResult,
        context: Optional[Dict[str, Any]],
    ):
        """Execute tools in parallel"""
        all_tools = [decision.primary_tool] + decision.secondary_tools

        # Create execution tasks
        tasks = []
        for tool_decision in all_tools:
            task = self._execute_tool_async(
                tool_decision, context, decision.engagement_id
            )
            tasks.append((tool_decision.tool_name, task))

        # Execute all tasks
        for tool_name, task in tasks:
            tool_start_time = time.time()
            try:
                result = await task
                execution_time = (time.time() - tool_start_time) * 1000

                execution_result.tool_results[tool_name] = result
                execution_result.tool_execution_times[tool_name] = execution_time

                if not result.success:
                    execution_result.errors.append(
                        f"Tool {tool_name} failed: {result.error_message}"
                    )

            except Exception as e:
                execution_time = (time.time() - tool_start_time) * 1000
                execution_result.tool_execution_times[tool_name] = execution_time
                execution_result.errors.append(f"Tool {tool_name} exception: {str(e)}")

    async def _execute_sequential_tools(
        self,
        decision: ToolDecisionDocument,
        execution_result: ToolDecisionExecutionResult,
        context: Optional[Dict[str, Any]],
    ):
        """Execute tools in sequence"""
        all_tools = [decision.primary_tool] + decision.secondary_tools

        # Execute in order
        for tool_decision in all_tools:
            tool_start_time = time.time()

            result = await self.tool_registry.execute_tool(
                tool_name=tool_decision.tool_name,
                parameters={k: v.value for k, v in tool_decision.parameters.items()},
                context=context,
                engagement_id=decision.engagement_id,
            )

            execution_time = (time.time() - tool_start_time) * 1000

            execution_result.tool_results[tool_decision.tool_name] = result
            execution_result.tool_execution_times[tool_decision.tool_name] = (
                execution_time
            )

            if not result.success:
                execution_result.errors.append(
                    f"Tool {tool_decision.tool_name} failed: {result.error_message}"
                )
                # For sequential execution, failure of one tool may stop the chain
                break

    async def _execute_conditional_tool(
        self,
        decision: ToolDecisionDocument,
        execution_result: ToolDecisionExecutionResult,
        context: Optional[Dict[str, Any]],
    ):
        """Execute tools based on conditions"""
        # This is a placeholder for conditional logic
        # In practice, conditions would be evaluated from the decision document
        await self._execute_single_tool(decision, execution_result, context)

    async def _execute_tool_async(
        self,
        tool_decision: ToolSelectionDecision,
        context: Optional[Dict[str, Any]],
        engagement_id: Optional[str],
    ) -> ToolExecutionResult:
        """Execute a tool asynchronously"""
        return await self.tool_registry.execute_tool(
            tool_name=tool_decision.tool_name,
            parameters={k: v.value for k, v in tool_decision.parameters.items()},
            context=context,
            engagement_id=engagement_id,
        )

    async def _aggregate_results(
        self,
        tool_results: Dict[str, ToolExecutionResult],
        decision: ToolDecisionDocument,
    ) -> Dict[str, Any]:
        """Aggregate results from multiple tool executions"""
        aggregated = {
            "total_tools_executed": len(tool_results),
            "successful_tools": len([r for r in tool_results.values() if r.success]),
            "failed_tools": len([r for r in tool_results.values() if not r.success]),
            "results_by_tool": {},
            "combined_confidence": 0.0,
        }

        confidences = []
        for tool_name, result in tool_results.items():
            aggregated["results_by_tool"][tool_name] = {
                "success": result.success,
                "result": result.result,
                "confidence": result.confidence_score,
                "execution_time_ms": result.execution_time_ms,
            }

            if result.success:
                confidences.append(result.confidence_score)

        # Calculate combined confidence (average of successful tools)
        if confidences:
            aggregated["combined_confidence"] = sum(confidences) / len(confidences)

        return aggregated

    async def _log_execution_event(
        self,
        decision: ToolDecisionDocument,
        execution_result: ToolDecisionExecutionResult,
        event_type: str,
        additional_data: Optional[Dict[str, Any]] = None,
    ):
        """Log execution event to context stream"""
        event_data = {
            "decision_id": decision.decision_id,
            "execution_id": execution_result.execution_id,
            "decision_type": decision.decision_type.value,
            "status": execution_result.status.value,
            "tool_count": len(execution_result.tool_results),
            **(additional_data or {}),
        }

        payload = {**event_data}
        if decision.phase:
            payload.setdefault("phase", decision.phase)
        latency = payload.pop("total_time_ms", None)
        self.tool_events.execution(
            decision.primary_tool.tool_name,
            action=event_type,
            latency_ms=latency,
            **payload,
        )


class ToolDecisionFramework:
    """
    Main framework coordinating LLM decision generation and deterministic execution

    This implements the tools-as-structured-decisions pattern:
    1. LLM generates structured ToolDecisionDocument
    2. Framework validates the decision
    3. DeterministicToolExecutor executes with full traceability
    """

    def __init__(
        self,
        llm_client: UnifiedLLMClient,
        tool_registry: MCPToolRegistry,
        context_stream: UnifiedContextStream,
        context_manager: IncrementalContextManager,
    ):
        self.logger = logging.getLogger(__name__)

        # Core components
        self.llm_client = llm_client
        self.tool_registry = tool_registry
        self.context_stream = context_stream
        self.context_manager = context_manager
        self.tool_events = ToolEventEmitter(
            context_stream, default_metadata={"component": "ToolDecisionFramework"}
        )

        # Specialized components
        self.validator = ToolDecisionValidator(tool_registry)
        self.executor = DeterministicToolExecutor(tool_registry, context_stream)
        self.cache = None  # get_performance_cache()

        # Framework state
        self.decision_history: Dict[str, ToolDecisionDocument] = {}
        self.execution_history: Dict[str, ToolDecisionExecutionResult] = {}

        self.logger.info("üõ†Ô∏è ToolDecisionFramework initialized")

    async def validate_available_tools(
        self, tools: List[str]
    ) -> ToolDecisionValidationResult:
        """Validate that tools are available and properly configured"""
        passed_checks = []
        failed_checks = []
        warnings = []

        available_tools = self.tool_registry.get_available_tools()

        for tool in tools:
            if tool in available_tools:
                passed_checks.append(f"Tool '{tool}' is available")
            else:
                failed_checks.append(f"Tool '{tool}' is not available")

        # Check for unknown tools
        for tool in tools:
            if tool not in [
                "search",
                "analyze",
                "synthesize",
                "research",
                "systems_thinking_analysis",
                "mece_problem_structuring",
            ]:
                warnings.append(f"Unknown tool type: {tool}")

        is_valid = len(failed_checks) == 0
        validation_score = len(passed_checks) / max(len(tools), 1)

        return ToolDecisionValidationResult(
            is_valid=is_valid,
            validation_score=validation_score,
            passed_checks=passed_checks,
            failed_checks=failed_checks,
            warnings=warnings,
            risk_level="low" if is_valid else "medium",
            requires_human_approval=not is_valid,
        )

    async def execute_tool_decision_cycle(
        self,
        query: str,
        context: Dict[str, Any],
        engagement_id: Optional[str] = None,
        require_human_approval: bool = False,
    ) -> ToolDecisionExecutionResult:
        """
        Execute complete tool decision cycle:
        1. Generate decision document with LLM
        2. Validate decision
        3. Execute deterministically
        4. Return results
        """
        try:
            # Step 1: Generate decision document
            self.logger.info(f"ü§î Generating tool decision for query: {query[:100]}...")
            decision_doc = await self.generate_tool_decision(
                query=query, context=context, engagement_id=engagement_id
            )

            # Step 2: Validate decision
            self.logger.info(f"‚úÖ Validating tool decision: {decision_doc.decision_id}")
            validation_result = await self.validator.validate_decision(
                decision_doc, context
            )

            if not validation_result.is_valid:
                raise ValueError(
                    f"Invalid tool decision: {', '.join(validation_result.failed_checks)}"
                )

            # Step 3: Check human approval requirement
            if require_human_approval or validation_result.requires_human_approval:
                self.logger.info(
                    f"üôã Human approval required for decision: {decision_doc.decision_id}"
                )
                # In a real implementation, this would pause and wait for human input
                # For now, we'll assume approval
                await self._log_human_approval_request(decision_doc, validation_result)

            # Step 4: Execute decision
            self.logger.info(f"‚öôÔ∏è Executing tool decision: {decision_doc.decision_id}")
            execution_result = await self.executor.execute_decision(
                decision=decision_doc,
                context=context,
                validation_result=validation_result,
            )

            # Step 5: Store results
            self.decision_history[decision_doc.decision_id] = decision_doc
            self.execution_history[decision_doc.decision_id] = execution_result

            self.logger.info(
                f"üéØ Tool decision cycle completed: {decision_doc.decision_id} | "
                f"Status: {execution_result.status.value} | "
                f"Time: {execution_result.total_execution_time_ms:.1f}ms"
            )

            return execution_result

        except Exception as e:
            self.logger.error(f"‚ùå Tool decision cycle failed: {e}")
            # Return failed execution result
            return ToolDecisionExecutionResult(
                decision_id="unknown",
                execution_id=f"failed_{int(time.time() * 1000)}",
                status=ToolDecisionStatus.FAILED,
                errors=[str(e)],
            )

    async def generate_tool_decision(
        self, query: str, context: Dict[str, Any], engagement_id: Optional[str] = None
    ) -> ToolDecisionDocument:
        """Generate structured tool decision using LLM"""

        # Get available tools
        available_tools = self.tool_registry.list_tools()

        # Prepare context for LLM
        llm_context = await self._prepare_llm_context(query, context, available_tools)

        # Generate decision using LLM
        decision_prompt = self._build_decision_prompt(
            query, llm_context, available_tools
        )

        llm_response = await self.llm_client.generate_response(
            prompt=decision_prompt,
            context=context,
            model_preference="reasoning",  # Use reasoning model for decisions
        )

        # Parse LLM response into structured decision document
        decision_doc = await self._parse_llm_decision_response(
            llm_response, query, context, engagement_id
        )

        try:
            self.tool_events.decision(
                decision_doc.primary_tool.tool_name,
                selection_reasoning=decision_doc.decision_reasoning,
                alternatives=decision_doc.primary_tool.alternative_tools,
                decision_id=decision_doc.decision_id,
                decision_type=decision_doc.decision_type.value,
                confidence=decision_doc.confidence_level,
            )
        except Exception:
            self.logger.debug("Tool decision event emission failed", exc_info=True)

        return decision_doc

    def _build_decision_prompt(
        self, query: str, context: Dict[str, Any], available_tools: List[Any]
    ) -> str:
        """Build prompt for LLM decision generation"""

        tools_description = "\n".join(
            [
                f"- {tool.name}: {tool.description} (Category: {tool.category.value})"
                for tool in available_tools
            ]
        )

        return f"""
You are a tool decision specialist. Your task is to generate a structured decision document for executing tools to answer the user's query.

QUERY: {query}

AVAILABLE TOOLS:
{tools_description}

CURRENT CONTEXT:
- Engagement phase: {context.get('phase', 'unknown')}
- Previous results: {len(context.get('previous_results', []))} items
- User preferences: {context.get('user_preferences', 'none specified')}

DECISION REQUIREMENTS:
1. Select the most appropriate tool(s) to answer the query
2. Specify exact parameters for each tool
3. Provide reasoning for your decisions
4. Consider execution order and dependencies
5. Assess risks and provide mitigation strategies

RESPONSE FORMAT (JSON):
{{
    "decision_type": "single_tool|parallel_tools|sequential_tools",
    "primary_tool": {{
        "tool_name": "selected_tool_name",
        "confidence": 0.85,
        "reasoning": "Why this tool was selected",
        "parameters": {{
            "param_name": {{
                "value": "parameter_value",
                "confidence": 0.9,
                "source": "llm|context|default",
                "reasoning": "Why this parameter value"
            }}
        }},
        "expected_output_type": "analysis|data|summary",
        "success_criteria": ["criterion1", "criterion2"]
    }},
    "secondary_tools": [],
    "execution_strategy": "immediate|deferred|conditional",
    "context_analysis": "Analysis of the current context",
    "decision_reasoning": "Overall reasoning for this decision",
    "confidence_level": 0.85,
    "validation_checks": ["check1", "check2"],
    "timeout_seconds": 120
}}

Generate your tool decision now:
"""

    async def _prepare_llm_context(
        self, query: str, context: Dict[str, Any], available_tools: List[Any]
    ) -> Dict[str, Any]:
        """Prepare enriched context for LLM decision making"""

        # Get recent context events
        recent_events = await self.context_stream.get_recent_events(limit=10)

        # Get incremental context updates
        context_updates = await self.context_manager.get_recent_updates()

        return {
            "query": query,
            "original_context": context,
            "recent_events": [asdict(event) for event in recent_events],
            "context_updates": [asdict(update) for update in context_updates],
            "available_tools_count": len(available_tools),
            "framework_capabilities": {
                "single_tool_execution": True,
                "parallel_execution": True,
                "sequential_execution": True,
                "conditional_execution": True,
            },
        }

    async def _parse_llm_decision_response(
        self,
        llm_response: Dict[str, Any],
        query: str,
        context: Dict[str, Any],
        engagement_id: Optional[str],
    ) -> ToolDecisionDocument:
        """Parse LLM response into structured decision document"""

        try:
            # Extract decision data from LLM response
            response_text = llm_response.get("content", "")

            # Try to parse as JSON (simplified for this implementation)
            # In practice, you'd use more robust JSON extraction
            decision_data = json.loads(response_text)

            # Create decision document
            decision_id = str(uuid.uuid4())

            # Parse primary tool decision
            primary_tool_data = decision_data["primary_tool"]
            primary_tool = ToolSelectionDecision(
                tool_name=primary_tool_data["tool_name"],
                confidence=primary_tool_data.get("confidence", 0.5),
                reasoning=primary_tool_data.get("reasoning", ""),
                parameters={},
                expected_output_type=primary_tool_data.get("expected_output_type"),
                success_criteria=primary_tool_data.get("success_criteria", []),
            )

            # Parse parameters
            for param_name, param_data in primary_tool_data.get(
                "parameters", {}
            ).items():
                if isinstance(param_data, dict):
                    primary_tool.parameters[param_name] = ToolParameterDecision(
                        parameter_name=param_name,
                        value=param_data.get("value"),
                        confidence=param_data.get("confidence", 0.5),
                        source=param_data.get("source", "llm"),
                        reasoning=param_data.get("reasoning", ""),
                    )
                else:
                    # Simple parameter format
                    primary_tool.parameters[param_name] = ToolParameterDecision(
                        parameter_name=param_name,
                        value=param_data,
                        confidence=0.5,
                        source="llm",
                        reasoning="Generated by LLM",
                    )

            # Create decision document
            decision_doc = ToolDecisionDocument(
                decision_id=decision_id,
                decision_type=ToolDecisionType(
                    decision_data.get("decision_type", "single_tool")
                ),
                strategy=ToolExecutionStrategy(
                    decision_data.get("execution_strategy", "immediate")
                ),
                primary_tool=primary_tool,
                context_analysis=decision_data.get("context_analysis", ""),
                decision_reasoning=decision_data.get("decision_reasoning", ""),
                confidence_level=decision_data.get("confidence_level", 0.5),
                validation_checks=decision_data.get("validation_checks", []),
                timeout_seconds=decision_data.get("timeout_seconds", 120),
                engagement_id=engagement_id,
                llm_model_used=llm_response.get("model"),
                token_usage=llm_response.get("usage"),
            )

            return decision_doc

        except Exception as e:
            self.logger.error(f"‚ùå Failed to parse LLM decision response: {e}")
            # Return a fallback decision document
            return self._create_fallback_decision(query, context, engagement_id)

    def _create_fallback_decision(
        self, query: str, context: Dict[str, Any], engagement_id: Optional[str]
    ) -> ToolDecisionDocument:
        """Create a fallback decision when LLM parsing fails"""

        decision_id = str(uuid.uuid4())

        # Use a simple systems thinking tool as fallback
        primary_tool = ToolSelectionDecision(
            tool_name="systems_thinking_analysis",
            confidence=0.6,
            reasoning="Fallback decision due to parsing error",
            parameters={
                "problem_statement": ToolParameterDecision(
                    parameter_name="problem_statement",
                    value=query,
                    confidence=0.8,
                    source="context",
                    reasoning="Using original query as problem statement",
                )
            },
            expected_output_type="analysis",
        )

        return ToolDecisionDocument(
            decision_id=decision_id,
            decision_type=ToolDecisionType.SINGLE_TOOL,
            strategy=ToolExecutionStrategy.IMMEDIATE,
            primary_tool=primary_tool,
            context_analysis="Fallback analysis due to parsing error",
            decision_reasoning="Generated fallback decision to ensure system continues functioning",
            confidence_level=0.6,
            timeout_seconds=60,
            engagement_id=engagement_id,
        )

    async def _log_human_approval_request(
        self,
        decision: ToolDecisionDocument,
        validation_result: ToolDecisionValidationResult,
    ):
        """Log human approval request event"""
        await self.context_stream.add_event(
            event_type=ContextEventType.HUMAN_INTERACTION,
            data={
                "decision_id": decision.decision_id,
                "approval_type": "tool_execution",
                "risk_level": validation_result.risk_level,
                "tools_to_execute": [decision.primary_tool.tool_name]
                + [t.tool_name for t in decision.secondary_tools],
                "estimated_time": decision.timeout_seconds,
                "confidence": decision.confidence_level,
            },
            metadata={"component": "ToolDecisionFramework", "requires_response": True},
        )

    def get_decision_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Get recent decision history"""
        decisions = list(self.decision_history.values())[-limit:]
        executions = {
            d.decision_id: self.execution_history.get(d.decision_id) for d in decisions
        }

        return [
            {
                "decision": asdict(decision),
                "execution": asdict(execution) if execution else None,
            }
            for decision, execution in zip(decisions, executions.values())
        ]

    def get_framework_metrics(self) -> Dict[str, Any]:
        """Get framework performance metrics"""
        total_decisions = len(self.decision_history)
        successful_executions = len(
            [
                e
                for e in self.execution_history.values()
                if e.status == ToolDecisionStatus.COMPLETED
            ]
        )

        return {
            "total_decisions_generated": total_decisions,
            "successful_executions": successful_executions,
            "success_rate": successful_executions / max(total_decisions, 1),
            "average_execution_time": sum(
                e.total_execution_time_ms for e in self.execution_history.values()
            )
            / max(len(self.execution_history), 1),
            "tool_registry_metrics": self.tool_registry.get_registry_metrics(),
            "framework_uptime": "running",  # Would track actual uptime
        }


# Global framework instance
_tool_decision_framework: Optional[ToolDecisionFramework] = None


def get_tool_decision_framework(
    llm_client: Optional[UnifiedLLMClient] = None,
    tool_registry: Optional[MCPToolRegistry] = None,
    context_stream: Optional[UnifiedContextStream] = None,
    context_manager: Optional[IncrementalContextManager] = None,
) -> ToolDecisionFramework:
    """Get or create global ToolDecisionFramework instance"""
    global _tool_decision_framework

    if _tool_decision_framework is None:
        # Import required dependencies
        try:
            from src.tools.mcp_mental_model_tools import get_mcp_tool_registry
            mcp_tool_registry = get_mcp_tool_registry()
        except ImportError:
            # MCP tools not available, use empty registry
            mcp_tool_registry = None

        from src.core.unified_context_stream import get_unified_context_stream, UnifiedContextStream
        from src.core.incremental_context_manager import IncrementalContextManager
        from src.integrations.llm.unified_client import get_unified_llm_client

        context_stream_instance = context_stream or get_unified_context_stream()
        _tool_decision_framework = ToolDecisionFramework(
            llm_client=llm_client or get_unified_llm_client(),
            tool_registry=tool_registry or mcp_tool_registry,
            context_stream=context_stream_instance,
            context_manager=context_manager
            or IncrementalContextManager(context_stream_instance),
        )

    return _tool_decision_framework
