"""
MetricsAggregator - Proof of Work Metrics Engine
Automatically aggregates key performance and complexity metrics for every engagement
Designed to parse UnifiedContextStream logs and generate comprehensive statistics
"""

import logging
from typing import Dict, List, Any, Optional, Set
from datetime import datetime
from dataclasses import dataclass

# Import UnifiedContextStream for event types
from src.core.unified_context_stream import ContextEvent, ContextEventType

logger = logging.getLogger(__name__)


@dataclass
class EngagementMetrics:
    """Data contract for final engagement metrics"""

    engagement_id: str
    llm_calls_count: int = 0
    perplexity_calls_count: int = 0
    total_characters_generated: int = 0
    total_tokens_used: int = 0
    total_cost_usd: float = 0.0  # Financial Transparency Enhancement
    nway_models_applied_count: int = 0
    cognitive_stages_count: int = 0
    processing_time_seconds: float = 0.0

    # Additional valuable metrics
    unique_tools_used: Set[str] = None
    unique_providers_used: Set[str] = None
    error_count: int = 0
    research_queries_count: int = 0
    reasoning_steps_count: int = 0
    stage0_enabled: bool = False
    stage0_variant: Optional[str] = None
    stage0_latency_ms: int = 0
    stage0_token_estimate: int = 0
    stage0_avg_tokens_per_consultant: int = 0
    stage0_insight_depth_score: Optional[float] = None

    def __post_init__(self):
        """Initialize sets after dataclass creation"""
        if self.unique_tools_used is None:
            self.unique_tools_used = set()
        if self.unique_providers_used is None:
            self.unique_providers_used = set()

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for database storage"""
        return {
            "llm_calls_count": self.llm_calls_count,
            "perplexity_calls_count": self.perplexity_calls_count,
            "total_characters_generated": self.total_characters_generated,
            "total_tokens_used": self.total_tokens_used,
            "total_cost_usd": self.total_cost_usd,  # Financial Transparency Enhancement
            "nway_models_applied_count": self.nway_models_applied_count,
            "cognitive_stages_count": self.cognitive_stages_count,
            "processing_time_seconds": self.processing_time_seconds,
            "unique_tools_used_count": len(self.unique_tools_used),
            "unique_providers_used_count": len(self.unique_providers_used),
            "error_count": self.error_count,
            "research_queries_count": self.research_queries_count,
            "reasoning_steps_count": self.reasoning_steps_count,
            "unique_tools_used": list(self.unique_tools_used),
            "unique_providers_used": list(self.unique_providers_used),
            "stage0_enabled": self.stage0_enabled,
            "stage0_variant": self.stage0_variant,
            "stage0_latency_ms": self.stage0_latency_ms,
            "stage0_token_estimate": self.stage0_token_estimate,
            "stage0_avg_tokens_per_consultant": self.stage0_avg_tokens_per_consultant,
            "stage0_insight_depth_score": self.stage0_insight_depth_score,
        }


class MetricsAggregator:
    """
    Dedicated service for calculating final "Proof of Work" statistics
    Subscribes to ENGAGEMENT_COMPLETED events and processes UnifiedContextStream logs
    """

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.MetricsAggregator")
        self.logger.info("ðŸŽ¯ MetricsAggregator service initialized")

    async def calculate_final_metrics(
        self, engagement_id: str, context_stream_log: List[ContextEvent]
    ) -> EngagementMetrics:
        """
        Master parser for UnifiedContextStream log
        Iterates through entire event log and calculates comprehensive metrics

        Args:
            engagement_id: Unique identifier for the engagement
            context_stream_log: Complete list of ContextEvent objects from engagement

        Returns:
            EngagementMetrics: Aggregated statistics for the engagement
        """
        start_time = datetime.now()

        metrics = EngagementMetrics(engagement_id=engagement_id)
        unique_nway_interactions = set()
        unique_phases = set()
        engagement_start_time = None
        engagement_end_time = None

        self.logger.info(
            f"ðŸ“Š Starting metrics calculation for engagement {engagement_id}"
        )
        self.logger.info(f"ðŸ“‹ Processing {len(context_stream_log)} events")

        for event in context_stream_log:
            try:
                # Track engagement timing
                if event.event_type == ContextEventType.ENGAGEMENT_STARTED:
                    engagement_start_time = event.timestamp
                elif event.event_type == ContextEventType.ENGAGEMENT_COMPLETED:
                    engagement_end_time = event.timestamp

                # Count LLM calls
                if event.event_type == ContextEventType.LLM_PROVIDER_RESPONSE:
                    metrics.llm_calls_count += 1

                    # Extract characters generated
                    response_length = event.data.get("response_length", 0)
                    if response_length == 0 and "content" in event.data:
                        response_length = len(str(event.data["content"]))
                    metrics.total_characters_generated += response_length

                    # Extract tokens used
                    tokens_used = event.data.get("tokens_used", 0)
                    if tokens_used == 0 and "token_count" in event.data:
                        tokens_used = event.data["token_count"]
                    metrics.total_tokens_used += tokens_used

                    # Extract cost (Financial Transparency Enhancement)
                    cost_usd = event.data.get("cost_usd", 0.0)
                    if cost_usd == 0.0 and "cost" in event.data:
                        cost_usd = float(event.data.get("cost", 0.0))
                    metrics.total_cost_usd += cost_usd

                    # Track unique providers
                    provider = event.data.get(
                        "provider", event.data.get("model", "unknown")
                    )
                    metrics.unique_providers_used.add(provider)

                # Count Perplexity calls
                elif event.event_type == ContextEventType.TOOL_CALL_COMPLETE:
                    tool_name = event.data.get("tool_name", "")
                    if "perplexity" in tool_name.lower():
                        metrics.perplexity_calls_count += 1

                    # Track unique tools
                    metrics.unique_tools_used.add(tool_name)

                # Count Perplexity searches (alternative event type)
                elif event.event_type == ContextEventType.PERPLEXITY_SEARCH_COMPLETE:
                    metrics.perplexity_calls_count += 1
                    metrics.unique_tools_used.add("perplexity_research")

                # Count N-Way models applied
                elif event.event_type == ContextEventType.NWAY_CLUSTER_ACTIVATED:
                    interaction_id = event.data.get("interaction_id", event.event_id)
                    if interaction_id not in unique_nway_interactions:
                        unique_nway_interactions.add(interaction_id)
                        metrics.nway_models_applied_count += 1

                # Track cognitive stages/phases
                if "phase" in event.data:
                    phase = event.data["phase"]
                    if phase:
                        unique_phases.add(str(phase).lower())

                # Count errors
                elif event.event_type == ContextEventType.ERROR_OCCURRED:
                    metrics.error_count += 1

                # Count research queries
                elif event.event_type == ContextEventType.RESEARCH_QUERY:
                    metrics.research_queries_count += 1

                # Count reasoning steps
                elif event.event_type == ContextEventType.REASONING_STEP:
                    metrics.reasoning_steps_count += 1

                elif event.event_type == ContextEventType.DEPTH_ENRICHMENT_METRICS:
                    metrics.stage0_enabled = bool(event.data.get("enabled", metrics.stage0_enabled))
                    metrics.stage0_variant = event.data.get("variant", metrics.stage0_variant)
                    metrics.stage0_latency_ms = int(event.data.get("latency_ms", metrics.stage0_latency_ms or 0))
                    metrics.stage0_token_estimate = int(
                        event.data.get("total_token_estimate", metrics.stage0_token_estimate or 0)
                    )
                    metrics.stage0_avg_tokens_per_consultant = int(
                        event.data.get(
                            "avg_tokens_per_consultant",
                            metrics.stage0_avg_tokens_per_consultant or 0,
                        )
                    )
                    if "insight_depth_score" in event.data:
                        metrics.stage0_insight_depth_score = event.data.get(
                            "insight_depth_score"
                        )

            except Exception as e:
                self.logger.warning(f"âš ï¸ Error processing event {event.event_id}: {e}")
                continue

        # Calculate final cognitive stages count
        metrics.cognitive_stages_count = len(unique_phases)

        # Calculate processing time
        if engagement_start_time and engagement_end_time:
            metrics.processing_time_seconds = (
                engagement_end_time - engagement_start_time
            ).total_seconds()

        calculation_time = (datetime.now() - start_time).total_seconds()

        self.logger.info(f"âœ… Metrics calculation completed in {calculation_time:.3f}s")
        self.logger.info(
            f"ðŸ“ˆ Final metrics: {metrics.llm_calls_count} LLM calls, {metrics.perplexity_calls_count} Perplexity calls"
        )
        self.logger.info(
            f"ðŸ“ Generated {metrics.total_characters_generated:,} characters using {metrics.total_tokens_used:,} tokens"
        )
        self.logger.info(
            f"ðŸ’° Total cost: ${metrics.total_cost_usd:.6f} USD"
        )  # Financial Transparency Enhancement
        self.logger.info(
            f"ðŸ§  Applied {metrics.nway_models_applied_count} N-way models across {metrics.cognitive_stages_count} cognitive stages"
        )

        return metrics

    def calculate_platform_wide_stats(
        self, all_engagement_metrics: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Calculate platform-wide aggregate statistics from all engagement metrics
        Used for landing page display and marketing purposes

        Args:
            all_engagement_metrics: List of proof_of_work_stats dictionaries from database

        Returns:
            Dictionary with platform-wide aggregate statistics
        """
        if not all_engagement_metrics:
            return {
                "total_llm_calls": 0,
                "total_perplexity_calls": 0,
                "total_characters_generated": 0,
                "total_tokens_used": 0,
                "total_cost_usd": 0.0,  # Financial Transparency Enhancement
                "total_nway_models_applied": 0,
                "total_cognitive_stages": 0,
                "total_engagements_processed": 0,
                "average_processing_time_seconds": 0.0,
                "mental_models_library_size": 100,  # Vanity stat - our library size
                "unique_analysis_frameworks": 25,  # Vanity stat - framework count
            }

        # Aggregate all metrics
        total_stats = {
            "total_llm_calls": sum(
                m.get("llm_calls_count", 0) for m in all_engagement_metrics
            ),
            "total_perplexity_calls": sum(
                m.get("perplexity_calls_count", 0) for m in all_engagement_metrics
            ),
            "total_characters_generated": sum(
                m.get("total_characters_generated", 0) for m in all_engagement_metrics
            ),
            "total_tokens_used": sum(
                m.get("total_tokens_used", 0) for m in all_engagement_metrics
            ),
            "total_cost_usd": sum(
                m.get("total_cost_usd", 0.0) for m in all_engagement_metrics
            ),  # Financial Transparency Enhancement
            "total_nway_models_applied": sum(
                m.get("nway_models_applied_count", 0) for m in all_engagement_metrics
            ),
            "total_cognitive_stages": sum(
                m.get("cognitive_stages_count", 0) for m in all_engagement_metrics
            ),
            "total_engagements_processed": len(all_engagement_metrics),
            "average_processing_time_seconds": sum(
                m.get("processing_time_seconds", 0) for m in all_engagement_metrics
            )
            / len(all_engagement_metrics),
            # Vanity stats that reflect our capabilities
            "mental_models_library_size": 100,
            "unique_analysis_frameworks": 25,
            "cognitive_diversity_score": 4.2,  # Out of 5.0
            "system_reliability_score": 99.7,  # Percentage uptime
        }

        self.logger.info(
            f"ðŸŒ Platform-wide stats calculated: {total_stats['total_engagements_processed']} engagements processed"
        )

        return total_stats


# Global instance for application use
_metrics_aggregator: Optional[MetricsAggregator] = None


def get_metrics_aggregator() -> MetricsAggregator:
    """Get or create the global metrics aggregator instance"""
    global _metrics_aggregator
    if _metrics_aggregator is None:
        _metrics_aggregator = MetricsAggregator()
    return _metrics_aggregator


# Convenience functions for easy integration
async def aggregate_engagement_metrics(
    engagement_id: str, context_events: List[ContextEvent]
) -> EngagementMetrics:
    """Convenience function to aggregate metrics for an engagement"""
    aggregator = get_metrics_aggregator()
    return await aggregator.calculate_final_metrics(engagement_id, context_events)


def calculate_platform_stats(
    engagement_metrics_list: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """Convenience function to calculate platform-wide statistics"""
    aggregator = get_metrics_aggregator()
    return aggregator.calculate_platform_wide_stats(engagement_metrics_list)
